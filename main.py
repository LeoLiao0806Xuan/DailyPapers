import requests
import json
from datetime import datetime, timedelta
import time
import config

# å…³é—­SSLè­¦å‘Šï¼ˆé¿å…è¯ä¹¦é—®é¢˜ï¼‰
import warnings
from urllib3.exceptions import InsecureRequestWarning
warnings.simplefilter('ignore', InsecureRequestWarning)
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# ------------------------------
# å¤šè½®æŠ“å–è®ºæ–‡ï¼ˆä¿®å¤400é”™è¯¯ï¼‰
# ------------------------------
def crawl_papers():
    papers = []
    print(f"ğŸ” å¼€å§‹æŠ“å–ã€{config.SEARCH_KEYWORDS}ã€‘é¢†åŸŸè¿‘{config.TIME_RANGE_DAYS}å¤©è®ºæ–‡ï¼ˆå¤šè½®æŠ“å–ï¼‰...")
    
    # æ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼ˆæ ¼å¼ä¸¥æ ¼ç¬¦åˆCrossRefè¦æ±‚ï¼‰
    from_date = (datetime.now() - timedelta(days=config.TIME_RANGE_DAYS)).strftime("%Y-%m-%d")
    base_url = "https://api.crossref.org/works"
    
    # å¤šè½®æŠ“å–ï¼šæ‹†åˆ†æ€»æ•°é‡ä¸ºå¤šä¸ªæ‰¹æ¬¡
    total_batches = config.TOTAL_CRAWL_COUNT // config.CRAWL_BATCH_SIZE
    for batch in range(total_batches):
        offset = batch * config.CRAWL_BATCH_SIZE
        
        # ä¿®å¤ï¼šåˆ†æ­¥æ„å»ºfilterå‚æ•°ï¼Œé¿å…æ ¼å¼é”™è¯¯
        filter_parts = [
            f"from-pub-date:{from_date}",
            "type:journal-article"
        ]
        # ä»…å½“è¯­è¨€å‚æ•°éç©ºæ—¶æ·»åŠ ï¼ˆé¿å…ç©ºå€¼å¯¼è‡´æ ¼å¼é”™è¯¯ï¼‰
        if config.LANGUAGE_SCOPE.strip():
            filter_parts.append(f"language:{config.LANGUAGE_SCOPE.strip()}")
        
        # é€šç”¨æŸ¥è¯¢å‚æ•°ï¼ˆä¿®å¤æ ¼å¼é—®é¢˜ï¼‰
        params = {
            "query": config.SEARCH_KEYWORDS.strip(),  # å»é™¤é¦–å°¾ç©ºæ ¼
            "filter": ",".join(filter_parts),         # æ­£ç¡®æ‹¼æ¥filter
            "sort": "is-referenced-by-count",         # æŒ‰å¼•ç”¨é‡é™åº
            "order": "desc",
            "rows": config.CRAWL_BATCH_SIZE,
            "offset": offset,
            "mailto": "your-email@example.com"        # æ·»åŠ mailtoï¼Œé¿å…APIé™æµï¼ˆå¯é€‰ï¼‰
        }
        
        try:
            # å‘é€è¯·æ±‚ï¼ˆç®€åŒ–Headerï¼Œé¿å…è¢«æ‹¦æˆªï¼‰
            response = requests.get(
                base_url,
                params=params,
                timeout=25,
                headers={"User-Agent": "Mozilla/5.0"},
                verify=False
            )
            
            if response.status_code == 200:
                data = response.json()
                batch_items = data.get('message', {}).get('items', [])
                print(f"ğŸ“Œ ç¬¬{batch+1}è½®ï¼šæŠ“å–åˆ° {len(batch_items)} ç¯‡åŸºç¡€è®ºæ–‡")
                
                # æå–è®ºæ–‡æ ¸å¿ƒä¿¡æ¯ï¼ˆé€šç”¨å­—æ®µï¼‰
                for item in batch_items:
                    # å¤„ç†å‘è¡¨æ—¶é—´ï¼ˆå…¼å®¹ä¸åŒæ ¼å¼ï¼‰
                    pub_date_parts = item.get('published', {}).get('date-parts', [['æœªçŸ¥æ—¶é—´']])[0]
                    pub_date = '-'.join([str(p) for p in pub_date_parts[:3]])  # å–å¹´-æœˆ-æ—¥
                    
                    papers.append({
                        "title": item.get('title', ['æœªçŸ¥æ ‡é¢˜'])[0],
                        "link": item.get('URL', '#'),
                        "journal": item.get('container-title', ['æœªçŸ¥æœŸåˆŠ'])[0],
                        "citations": item.get('is-referenced-by-count', 0),
                        "published": pub_date,
                        "authors": ", ".join([f"{auth.get('family', '')} {auth.get('given', '')}".strip() 
                                             for auth in item.get('author', [])[:3]]) or "æœªçŸ¥ä½œè€…"
                    })
            else:
                print(f"âŒ ç¬¬{batch+1}è½®ï¼šè¯·æ±‚å¤±è´¥ï¼ˆçŠ¶æ€ç : {response.status_code}ï¼‰")
                print(f"âŒ é”™è¯¯è¯¦æƒ…ï¼š{response.text[:200]}")  # æ‰“å°é”™è¯¯è¯¦æƒ…ï¼Œæ–¹ä¾¿æ’æŸ¥
                break
            
            # è½®æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…APIé™æµ
            time.sleep(config.CRAWL_DELAY)
        
        except Exception as e:
            print(f"âŒ ç¬¬{batch+1}è½®ï¼šæŠ“å–å¼‚å¸¸ - {str(e)[:60]}")
            break
    
    # æœ¬åœ°ç­›é€‰ï¼šä¼˜å…ˆé«˜å½±å“å› å­æœŸåˆŠ + æŒ‰å¼•ç”¨é‡æ’åº
    high_impact_papers = [p for p in papers if p['journal'] in config.HIGH_IMPACT_JOURNALS]
    other_papers = [p for p in papers if p['journal'] not in config.HIGH_IMPACT_JOURNALS]
    
    # åˆå¹¶æ’åºï¼šé«˜å½±å“å› å­æœŸåˆŠåœ¨å‰ï¼ŒåŒç»„å†…æŒ‰å¼•ç”¨é‡é™åº
    sorted_papers = sorted(high_impact_papers, key=lambda x: x['citations'], reverse=True) + \
                    sorted(other_papers, key=lambda x: x['citations'], reverse=True)
    
    print(f"âœ… æ€»è®¡ç­›é€‰å‡º {len(sorted_papers)} ç¯‡ç›¸å…³è®ºæ–‡ï¼ˆé«˜å½±å“å› å­æœŸåˆŠä¼˜å…ˆï¼‰")
    return sorted_papers

# ------------------------------
# ä¸»æµç¨‹ï¼ˆé€šç”¨é€»è¾‘ï¼‰
# ------------------------------
if __name__ == "__main__":
    # æ‰“å°æŠ“å–ä¿¡æ¯
    print("="*60)
    print(f"ğŸ“… é€šç”¨è®ºæ–‡æŠ“å– {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ” é¢†åŸŸï¼š{config.SEARCH_KEYWORDS} | è¿‘{config.TIME_RANGE_DAYS}å¤© | å¼•ç”¨é‡â‰¥{config.MIN_CITATION_THRESHOLD}")
    print("="*60)

    # 1. å¤šè½®æŠ“å–è®ºæ–‡
    all_papers = crawl_papers()
    if not all_papers:
        print("\nâŒ æœªæŠ“å–åˆ°ç¬¦åˆæ¡ä»¶çš„è®ºæ–‡")
        exit()

    # 2. ç­›é€‰é«˜å¼•ç”¨è®ºæ–‡ï¼ˆâ‰¥é˜ˆå€¼ï¼‰
    high_citation_papers = [p for p in all_papers if p['citations'] >= config.MIN_CITATION_THRESHOLD]
    print(f"\nğŸ“Š å¼•ç”¨é‡â‰¥{config.MIN_CITATION_THRESHOLD}çš„è®ºæ–‡ï¼š{len(high_citation_papers)} ç¯‡")

    # 3. é€‰æ‹©æŒ‡å®šæ•°é‡è®ºæ–‡
    selected_papers = high_citation_papers[:config.PAPER_COUNT] if high_citation_papers else all_papers[:config.PAPER_COUNT]

    # 4. ç”ŸæˆGitHub Issueå†…å®¹
    print(f"\nğŸš€ æ¨é€ {len(selected_papers)} ç¯‡é«˜å½±å“åŠ›è®ºæ–‡åˆ°GitHub...")
    issue_title = f"[{config.SEARCH_KEYWORDS}] é«˜å¼•ç”¨è®ºæ–‡æ¨è {datetime.now().strftime('%Y-%m-%d')}"
    issue_body = f"""# {config.SEARCH_KEYWORDS} é¢†åŸŸé«˜å½±å“åŠ›è®ºæ–‡æ¨è ({datetime.now().strftime('%Y-%m-%d')})

### ğŸ“š ç­›é€‰è§„åˆ™
- **é¢†åŸŸ**ï¼š{config.SEARCH_KEYWORDS}
- **æ—¶é—´**ï¼šè¿‘ {config.TIME_RANGE_DAYS} å¤©å‘è¡¨
- **æ¥æº**ï¼šä¼˜å…ˆé«˜å½±å“å› å­æœŸåˆŠï¼ˆ{len(config.HIGH_IMPACT_JOURNALS)}ä¸ªæ ¸å¿ƒåˆŠï¼‰
- **é—¨æ§›**ï¼šå¼•ç”¨é‡â‰¥{config.MIN_CITATION_THRESHOLD}
- **æ’åº**ï¼šé«˜å½±å“å› å­æœŸåˆŠä¼˜å…ˆï¼ŒåŒæœŸåˆŠå†…æŒ‰å¼•ç”¨é‡é™åº

---
"""

    # æ‹¼æ¥è®ºæ–‡åˆ—è¡¨
    for i, paper in enumerate(selected_papers):
        issue_body += f"""
## {i+1}. {paper['title']}
> æœŸåˆŠï¼š{paper['journal']} | å‘è¡¨æ—¶é—´ï¼š{paper['published']} | å¼•ç”¨é‡ï¼š{paper['citations']} | ä½œè€…ï¼š{paper['authors']}
- åŸæ–‡é“¾æ¥ï¼š{paper['link']}

---
"""

    # 5. æ¨é€è‡³GitHub Issue
    try:
        response = requests.post(
            f"https://api.github.com/repos/{config.REPO_OWNER}/{config.REPO_NAME}/issues",
            headers={
                "Authorization": f"token {config.GITHUB_TOKEN}",
                "Accept": "application/vnd.github.v3+json",
                "Content-Type": "application/json"
            },
            data=json.dumps({
                "title": issue_title,
                "body": issue_body
            }),
            timeout=25,
            verify=False
        )
        
        if response.status_code == 201:
            res_json = response.json()
            print(f"ğŸ‰ æ¨é€æˆåŠŸï¼Issueåœ°å€ï¼š{res_json['html_url']}")
        else:
            print(f"âŒ æ¨é€å¤±è´¥ï¼ˆçŠ¶æ€ç : {response.status_code}ï¼‰")
            print(f"âŒ é”™è¯¯è¯¦æƒ…ï¼š{response.text[:200]}")
    except Exception as e:
        print(f"âŒ æ¨é€å¼‚å¸¸ï¼š{str(e)[:60]}")